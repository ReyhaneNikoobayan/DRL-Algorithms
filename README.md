# DRL-Algorithms

# ğŸ§  Deep Reinforcement Learning Portfolio

A curated collection of **Deep Reinforcement Learning (DRL)** algorithms implemented **from scratch** using PyTorch and Gymnasium.  

The goals of this repository are to:

- ğŸ¯ Understand and implement core RL and DRL algorithms from scratch  
- ğŸ§© Build fundamental components (replay buffers, target networks, policy gradients, etc.) manually  
- ğŸ“ˆ Train and evaluate agents on classic control environments  
- ğŸ“š Provide clear, educational code for others studying reinforcement learning  

This repository includes **tabular methods**, **value-based deep RL**, and **policy-gradient methods**, tested on well-known Gymnasium environments such as:

- CartPole-v1  
- FrozenLake-v1  
- MountainCar-v0  
- LunarLander-v2  
- Acrobot-v1  

---

## ğŸŒŸ Why this project?

Reinforcement Learning is best learned through **hands-on implementation**.  
By coding each algorithm step-by-step, I aim to deeply understand how RL works behind the scenes.

This repository is an evolving portfolio and a demonstration of my progress in DRL.

---

# ğŸš• Taxi-v3 Q-Learning Project

This project trains an agent using **Q-learning** to solve the classic **Taxi-v3** environment from *OpenAI Gymnasium*. The agent learns optimal pickup and drop-off strategies through exploration, exploitation, and iterative updates to a Q-table.

ğŸ“„ **Source Code:**  
[Q-Learning Taxi Agent](https://github.com/ReyhaneNikoobayan/DRL-Algorithms/blob/main/q_learning.ipynb)


---

## ğŸ“Œ Features of This Implementation

* Q-learning with:

  * High learning rate: `alpha = 0.9`
  * Discount factor: `gamma = 0.95`
  * Epsilon-greedy action selection
  * Epsilon decay from 1.0 to 0.01
* Moving average reward plot saved automatically
* GIF video recordings (5 episodes) using the learned policy
* Organized project structure: saves results under `results/plots` and `results/videos`

---

## ğŸ“‚ Files Generated

Below are examples of the **actual results** produced by the code:

### ğŸ¯ **Training Performance (Moving Average Reward)**

The plot below shows the learning curve of the Taxi agent. The average reward increases over episodes as the agent learns an optimal policy.

**Preview:**
<img width="3000" height="1500" alt="moving_avg_reward" src="https://github.com/user-attachments/assets/1902a523-12b3-4bba-9778-8a61d7fab0c2" />


### ğŸï¸ **Trained Agent Performance (GIF Episodes)**

<div style="display: flex; gap: 10px;">
  <img src="https://github.com/user-attachments/assets/8a622882-da2d-4a10-8fc7-8ff52f7ab34d" width="230"/>
  <img src="https://github.com/user-attachments/assets/bd4e4b63-93eb-4cbc-a36a-9697c435b7e0" width="230"/>
  <img src="https://github.com/user-attachments/assets/29846e67-b0f1-4c86-bd67-5647ced157ad" width="230"/>
</div>

<div style="display: flex; gap: 10px; margin-top: 10px;">
  <img src="https://github.com/user-attachments/assets/e5655478-336f-4fa2-96de-13128424a440" width="230"/>
  <img src="https://github.com/user-attachments/assets/a4b50f1c-cedf-4be5-b662-7b9aa235b12a" width="230"/>
</div>



## ğŸ§  Qâ€‘Learning Formula Used

The Q-table is updated using:

```
Q(s, a) = (1 - Î±) * Q(s, a) + Î± * (reward + Î³ * max(Q(s')))
```

Where:

* **Î± (alpha)** = learning rate
* **Î³ (gamma)** = discount factor
* **s** = current state
* **a** = chosen action
* **s'** = next state

---

## â–¶ï¸ How the Training Works

1. Initialize q-table to zeros
2. For each episode:

   * Reset the environment
   * For up to 100 steps:

     * Choose action via epsilon-greedy
     * Take action â†’ receive reward and next state
     * Update Q-table
     * Break if the episode ends
3. Decay epsilon
4. Store total reward per episode

---

# ğŸ® FrozenLake DQN Project

This project implements **Deep Q-Network (DQN)** to solve the **FrozenLake-v1** environment from *OpenAI Gymnasium*. The agent learns to navigate the 4x4 FrozenLake map using reinforcement learning, experience replay, and a target network to stabilize training.

ğŸ“„ **Source Code:**
[FrozenLake DQN Agent](https://github.com/ReyhaneNikoobayan/DRL-Algorithms/blob/main/DQL-FrozenLake.ipynb)

---

## ğŸ“Œ Features of This Implementation

* Deep Q-Network (DQN) with:

  * Single hidden-layer feedforward neural network
  * Adam optimizer with learning rate: `0.001`
  * Experience replay buffer (`memory_size=1000`)
  * Target network synced every `sync_rate=10` steps
  * Discount factor: `gamma=0.9`
  * Epsilon-greedy policy with linear decay
* Moving average reward plot saved automatically
* GIF video recordings (episodes) using the learned policy
* Organized project structure: saves results under `results/plots` and `results/videos`

---

## ğŸ“‚ Files Generated

Below are examples of the **actual results** produced by the code:

### ğŸ¯ **Training Performance (Moving Average Reward)**

The plot below shows the learning curve of the FrozenLake agent. The average reward increases over episodes as the agent learns an optimal policy.

<img width="1674" height="920" alt="Screenshot 2025-12-11 185216" src="https://github.com/user-attachments/assets/aee8f461-f5e8-4cc5-88fa-95b109e56f90" />



### ğŸï¸ **Trained Agent Performance (GIF Episodes)**

![frozenlake_episode_0](https://github.com/user-attachments/assets/38042cd4-444d-48dd-a78d-4676085ab6d8)

---

## ğŸ§  DQN Formula Used

The Q-values are updated using the DQN method with a target network:

```
Q(s, a) = reward + Î³ * max(Q_target(s'))
```

Where:

* **Î³ (gamma)** = discount factor
* **s** = current state
* **a** = chosen action
* **s'** = next state
* **Q_target** = target network prediction

---

## â–¶ï¸ How the Training Works

1. Initialize **Policy** and **Target** networks
2. For each episode:

   * Reset the environment
   * Choose action using epsilon-greedy policy
   * Step through the environment â†’ store transition in memory
   * Update Policy network using sampled mini-batches from memory
   * Sync Target network every `sync_rate` steps
   * Decay epsilon gradually
3. Record total reward per episode
4. Save the trained model and learning curve plot

---

## â–¶ï¸ How Testing Works

1. Load the trained Policy network
2. Run agent for N episodes
3. Render each step and store frames
4. Save frames as GIFs for visualization


# ğŸš—â›°ï¸ MountainCar-v0 Deep Q-Learning Project

This project trains an agent using a **Deep Q-Network (DQN)** to solve the classic **MountainCar-v0** environment from *OpenAI Gymnasium*.  
The agent learns to climb the mountain by building momentum and optimizing long-term rewards using replay memory, target networks, and gradient-based Q-value updates.

ğŸ“„ **Source Code:**  
[mountaincar DQN Agent](https://github.com/ReyhaneNikoobayan/DRL-Algorithms/blob/main/DQL_mountaincar.ipynb)

---

## ğŸ“Œ Features of This Implementation

âœ” Fully implemented DQN with PyTorch  
âœ” Replay Memory for experience replay  
âœ” Target network for stable learning  
âœ” Epsilon-greedy exploration with decay  
âœ” Reward shaping for faster convergence  
âœ” GIF recording of trained agent  
âœ” Moving average reward plot  

### ğŸ”§ Hyperparameters Used

* Learning rate: `0.001`
* Discount factor (gamma): `0.9`
* Batch size: `32`
* Replay memory size: `10,000`
* Target network sync rate: `1000`
* Epsilon: `1.0 â†’ 0.01`

---

## ğŸ“‚ Files Generated by the Code

### ğŸ¯ Training Performance (Moving Average Reward)

This plot illustrates how the agent improves over time:

<img width="1200" height="600" alt="moving_average_rewards" src="https://github.com/user-attachments/assets/a8b333f4-eec9-4f25-ac5c-d488d83b1d82" />


---

### ğŸï¸ Trained Agent GIFs

Below are example GIFs produced during evaluation:

![mountain_car_episode_7](https://github.com/user-attachments/assets/d33f90d2-e794-4875-b85d-15791d2aa5d6)





---

## ğŸ§  DQN Learning Formula

The target Q-value is computed using:

Q_target = reward + Î³ * max(Q_target_network(s'))


And the loss function is:

Loss = MSE( Q_policy(s), Q_target )


---

## â–¶ï¸ How Training Works

1. Initialize policy & target networks  
2. Create replay memory  
3. For each episode:
   * Reset environment  
   * Select action using epsilon-greedy  
   * Execute action â†’ observe reward and next state  
   * Store transition in replay memory  
   * Sample batch and optimize policy network  
   * Sync target network every 1000 steps  
4. Save:
   * Trained model â†’ `MountainCar_dql.pt`
   * Reward plot â†’ `moving_average_rewards.png`
   * GIFs â†’ `results/videos/`

---

## â­ If you use this projectâ€¦

Please consider starring the repository â­  
It helps others discover this project.





